<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.3" />
<title>code.yolo.darknet API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>code.yolo.darknet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from __future__ import division

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
import cv2
import matplotlib.pyplot as plt
from yolo.util import count_parameters as count
from yolo.util import convert2cpu as cpu
from yolo.util import predict_transform


class test_net(nn.Module):
    def __init__(self, num_layers, input_size):
        super(test_net, self).__init__()
        self.num_layers = num_layers
        self.linear_1 = nn.Linear(input_size, 5)
        self.middle = nn.ModuleList([nn.Linear(5, 5) for x in range(num_layers)])
        self.output = nn.Linear(5, 2)

    def forward(self, x):
        x = x.view(-1)
        fwd = nn.Sequential(self.linear_1, *self.middle, self.output)
        return fwd(x)


def get_test_input():
    img = cv2.imread(&#34;dog-cycle-car.png&#34;)
    img = cv2.resize(img, (416, 416))
    img_ = img[:, :, ::-1].transpose((2, 0, 1))
    img_ = img_[np.newaxis, :, :, :] / 255.0
    img_ = torch.from_numpy(img_).float()
    img_ = Variable(img_)
    return img_


def parse_cfg(cfgfile):
    &#34;&#34;&#34;
    Takes a configuration file
    
    Returns a list of blocks. Each blocks describes a block in the neural
    network to be built. Block is represented as a dictionary in the list
    
    &#34;&#34;&#34;
    file = open(cfgfile, &#39;r&#39;)
    lines = file.read().split(&#39;\n&#39;)  # store the lines in a list
    lines = [x for x in lines if len(x) &gt; 0]  # get read of the empty lines
    lines = [x for x in lines if x[0] != &#39;#&#39;]
    lines = [x.rstrip().lstrip() for x in lines]

    block = {}
    blocks = []

    for line in lines:
        if line[0] == &#34;[&#34;:  # This marks the start of a new block
            if len(block) != 0:
                blocks.append(block)
                block = {}
            block[&#34;type&#34;] = line[1:-1].rstrip()
        else:
            key, value = line.split(&#34;=&#34;)
            block[key.rstrip()] = value.lstrip()
    blocks.append(block)

    return blocks


#    print(&#39;\n\n&#39;.join([repr(x) for x in blocks]))

import pickle as pkl


class MaxPoolStride1(nn.Module):
    def __init__(self, kernel_size):
        super(MaxPoolStride1, self).__init__()
        self.kernel_size = kernel_size
        self.pad = kernel_size - 1

    def forward(self, x):
        padded_x = F.pad(x, (0, self.pad, 0, self.pad), mode=&#34;replicate&#34;)
        pooled_x = nn.MaxPool2d(self.kernel_size, self.pad)(padded_x)
        return pooled_x


class EmptyLayer(nn.Module):
    def __init__(self):
        super(EmptyLayer, self).__init__()


class DetectionLayer(nn.Module):
    def __init__(self, anchors):
        super(DetectionLayer, self).__init__()
        self.anchors = anchors

    def forward(self, x, inp_dim, num_classes, confidence):
        x = x.data
        global CUDA
        prediction = x
        prediction = predict_transform(prediction, inp_dim, self.anchors, num_classes, confidence, CUDA)
        return prediction


class Upsample(nn.Module):
    def __init__(self, stride=2):
        super(Upsample, self).__init__()
        self.stride = stride

    def forward(self, x):
        stride = self.stride
        assert (x.data.dim() == 4)
        B = x.data.size(0)
        C = x.data.size(1)
        H = x.data.size(2)
        W = x.data.size(3)
        ws = stride
        hs = stride
        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H * stride, W * stride)
        return x


#

class ReOrgLayer(nn.Module):
    def __init__(self, stride=2):
        super(ReOrgLayer, self).__init__()
        self.stride = stride

    def forward(self, x):
        assert (x.data.dim() == 4)
        B, C, H, W = x.data.shape
        hs = self.stride
        ws = self.stride
        assert (H % hs == 0), &#34;The stride &#34; + str(self.stride) + &#34; is not a proper divisor of height &#34; + str(H)
        assert (W % ws == 0), &#34;The stride &#34; + str(self.stride) + &#34; is not a proper divisor of height &#34; + str(W)
        x = x.view(B, C, H // hs, hs, W // ws, ws).transpose(-2, -3).contiguous()
        x = x.view(B, C, H // hs * W // ws, hs, ws)
        x = x.view(B, C, H // hs * W // ws, hs * ws).transpose(-1, -2).contiguous()
        x = x.view(B, C, ws * hs, H // ws, W // ws).transpose(1, 2).contiguous()
        x = x.view(B, C * ws * hs, H // ws, W // ws)
        return x


def create_modules(blocks):
    net_info = blocks[0]  # Captures the information about the input and pre-processing

    module_list = nn.ModuleList()

    index = 0  # indexing blocks helps with implementing route  layers (skip connections)

    prev_filters = 3

    output_filters = []

    for x in blocks:
        module = nn.Sequential()

        if (x[&#34;type&#34;] == &#34;net&#34;):
            continue

        # If it&#39;s a convolutional layer
        if (x[&#34;type&#34;] == &#34;convolutional&#34;):
            # Get the info about the layer
            activation = x[&#34;activation&#34;]
            try:
                batch_normalize = int(x[&#34;batch_normalize&#34;])
                bias = False
            except:
                batch_normalize = 0
                bias = True

            filters = int(x[&#34;filters&#34;])
            padding = int(x[&#34;pad&#34;])
            kernel_size = int(x[&#34;size&#34;])
            stride = int(x[&#34;stride&#34;])

            if padding:
                pad = (kernel_size - 1) // 2
            else:
                pad = 0

            # Add the convolutional layer
            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)
            module.add_module(&#34;conv_{0}&#34;.format(index), conv)

            # Add the Batch Norm Layer
            if batch_normalize:
                bn = nn.BatchNorm2d(filters)
                module.add_module(&#34;batch_norm_{0}&#34;.format(index), bn)

            # Check the activation.
            # It is either Linear or a Leaky ReLU for YOLO
            if activation == &#34;leaky&#34;:
                activn = nn.LeakyReLU(0.1, inplace=True)
                module.add_module(&#34;leaky_{0}&#34;.format(index), activn)



        # If it&#39;s an upsampling layer
        # We use Bilinear2dUpsampling

        elif (x[&#34;type&#34;] == &#34;upsample&#34;):
            stride = int(x[&#34;stride&#34;])
            #            upsample = Upsample(stride)
            upsample = nn.Upsample(scale_factor=2, mode=&#34;nearest&#34;)
            module.add_module(&#34;upsample_{}&#34;.format(index), upsample)

        # If it is a route layer
        elif (x[&#34;type&#34;] == &#34;route&#34;):
            x[&#34;layers&#34;] = x[&#34;layers&#34;].split(&#39;,&#39;)

            # Start  of a route
            start = int(x[&#34;layers&#34;][0])

            # end, if there exists one.
            try:
                end = int(x[&#34;layers&#34;][1])
            except:
                end = 0

            # Positive anotation
            if start &gt; 0:
                start = start - index

            if end &gt; 0:
                end = end - index

            route = EmptyLayer()
            module.add_module(&#34;route_{0}&#34;.format(index), route)

            if end &lt; 0:
                filters = output_filters[index + start] + output_filters[index + end]
            else:
                filters = output_filters[index + start]



        # shortcut corresponds to skip connection
        elif x[&#34;type&#34;] == &#34;shortcut&#34;:
            from_ = int(x[&#34;from&#34;])
            shortcut = EmptyLayer()
            module.add_module(&#34;shortcut_{}&#34;.format(index), shortcut)


        elif x[&#34;type&#34;] == &#34;maxpool&#34;:
            stride = int(x[&#34;stride&#34;])
            size = int(x[&#34;size&#34;])
            if stride != 1:
                maxpool = nn.MaxPool2d(size, stride)
            else:
                maxpool = MaxPoolStride1(size)

            module.add_module(&#34;maxpool_{}&#34;.format(index), maxpool)

        # Yolo is the detection layer
        elif x[&#34;type&#34;] == &#34;yolo&#34;:
            mask = x[&#34;mask&#34;].split(&#34;,&#34;)
            mask = [int(x) for x in mask]

            anchors = x[&#34;anchors&#34;].split(&#34;,&#34;)
            anchors = [int(a) for a in anchors]
            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]
            anchors = [anchors[i] for i in mask]

            detection = DetectionLayer(anchors)
            module.add_module(&#34;Detection_{}&#34;.format(index), detection)



        else:
            print(&#34;Something I dunno&#34;)
            assert False

        module_list.append(module)
        prev_filters = filters
        output_filters.append(filters)
        index += 1

    return (net_info, module_list)


class Darknet(nn.Module):
    def __init__(self, cfgfile):
        super(Darknet, self).__init__()
        self.blocks = parse_cfg(cfgfile)
        self.net_info, self.module_list = create_modules(self.blocks)
        self.header = torch.IntTensor([0, 0, 0, 0])
        self.seen = 0

    def get_blocks(self):
        return self.blocks

    def get_module_list(self):
        return self.module_list

    def forward(self, x, CUDA):
        detections = []
        modules = self.blocks[1:]
        outputs = {}  # We cache the outputs for the route layer

        write = 0
        for i in range(len(modules)):

            module_type = (modules[i][&#34;type&#34;])
            if module_type == &#34;convolutional&#34; or module_type == &#34;upsample&#34; or module_type == &#34;maxpool&#34;:

                x = self.module_list[i](x)
                outputs[i] = x


            elif module_type == &#34;route&#34;:
                layers = modules[i][&#34;layers&#34;]
                layers = [int(a) for a in layers]

                if (layers[0]) &gt; 0:
                    layers[0] = layers[0] - i

                if len(layers) == 1:
                    x = outputs[i + (layers[0])]

                else:
                    if (layers[1]) &gt; 0:
                        layers[1] = layers[1] - i

                    map1 = outputs[i + layers[0]]
                    map2 = outputs[i + layers[1]]

                    x = torch.cat((map1, map2), 1)
                outputs[i] = x

            elif module_type == &#34;shortcut&#34;:
                from_ = int(modules[i][&#34;from&#34;])
                x = outputs[i - 1] + outputs[i + from_]
                outputs[i] = x



            elif module_type == &#39;yolo&#39;:

                anchors = self.module_list[i][0].anchors
                # Get the input dimensions
                inp_dim = int(self.net_info[&#34;height&#34;])

                # Get the number of classes
                num_classes = int(modules[i][&#34;classes&#34;])

                # Output the result
                x = x.data
                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)

                if type(x) == int:
                    continue

                if not write:
                    detections = x
                    write = 1

                else:
                    detections = torch.cat((detections, x), 1)

                outputs[i] = outputs[i - 1]

        try:
            return detections
        except:
            return 0

    def load_weights(self, weightfile):

        # Open the weights file
        fp = open(weightfile, &#34;rb&#34;)

        # The first 4 values are header information
        # 1. Major version number
        # 2. Minor Version Number
        # 3. Subversion number 
        # 4. IMages seen 
        header = np.fromfile(fp, dtype=np.int32, count=5)
        self.header = torch.from_numpy(header)
        self.seen = self.header[3]

        # The rest of the values are the weights
        # Let&#39;s load them up
        weights = np.fromfile(fp, dtype=np.float32)

        ptr = 0
        for i in range(len(self.module_list)):
            module_type = self.blocks[i + 1][&#34;type&#34;]

            if module_type == &#34;convolutional&#34;:
                model = self.module_list[i]
                try:
                    batch_normalize = int(self.blocks[i + 1][&#34;batch_normalize&#34;])
                except:
                    batch_normalize = 0

                conv = model[0]

                if (batch_normalize):
                    bn = model[1]

                    # Get the number of weights of Batch Norm Layer
                    num_bn_biases = bn.bias.numel()

                    # Load the weights
                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])
                    ptr += num_bn_biases

                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                    ptr += num_bn_biases

                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                    ptr += num_bn_biases

                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                    ptr += num_bn_biases

                    # Cast the loaded weights into dims of models weights.
                    bn_biases = bn_biases.view_as(bn.bias.data)
                    bn_weights = bn_weights.view_as(bn.weight.data)
                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)
                    bn_running_var = bn_running_var.view_as(bn.running_var)

                    # Copy the data to models
                    bn.bias.data.copy_(bn_biases)
                    bn.weight.data.copy_(bn_weights)
                    bn.running_mean.copy_(bn_running_mean)
                    bn.running_var.copy_(bn_running_var)

                else:
                    # Number of biases
                    num_biases = conv.bias.numel()

                    # Load the weights
                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])
                    ptr = ptr + num_biases

                    # reshape the loaded weights according to the dims of the models weights
                    conv_biases = conv_biases.view_as(conv.bias.data)

                    # Finally copy the data
                    conv.bias.data.copy_(conv_biases)

                # Let us load the weights for the Convolutional layers
                num_weights = conv.weight.numel()

                # Do the same as above for weights
                conv_weights = torch.from_numpy(weights[ptr:ptr + num_weights])
                ptr = ptr + num_weights

                conv_weights = conv_weights.view_as(conv.weight.data)
                conv.weight.data.copy_(conv_weights)

    def save_weights(self, savedfile, cutoff=0):

        if cutoff &lt;= 0:
            cutoff = len(self.blocks) - 1

        fp = open(savedfile, &#39;wb&#39;)

        # Attach the header at the top of the file
        self.header[3] = self.seen
        header = self.header

        header = header.numpy()
        header.tofile(fp)

        # Now, let us save the weights 
        for i in range(len(self.module_list)):
            module_type = self.blocks[i + 1][&#34;type&#34;]

            if (module_type) == &#34;convolutional&#34;:
                model = self.module_list[i]
                try:
                    batch_normalize = int(self.blocks[i + 1][&#34;batch_normalize&#34;])
                except:
                    batch_normalize = 0

                conv = model[0]

                if (batch_normalize):
                    bn = model[1]

                    # If the parameters are on GPU, convert them back to CPU
                    # We don&#39;t convert the parameter to GPU
                    # Instead. we copy the parameter and then convert it to CPU
                    # This is done as weight are need to be saved during training
                    cpu(bn.bias.data).numpy().tofile(fp)
                    cpu(bn.weight.data).numpy().tofile(fp)
                    cpu(bn.running_mean).numpy().tofile(fp)
                    cpu(bn.running_var).numpy().tofile(fp)


                else:
                    cpu(conv.bias.data).numpy().tofile(fp)

                # Let us save the weights for the Convolutional layers
                cpu(conv.weight.data).numpy().tofile(fp)

#
# dn = Darknet(&#39;cfg/yolov3.cfg&#39;)
# dn.load_weights(&#34;yolov3.weights&#34;)
# inp = get_test_input()
# a, interms = dn(inp)
# dn.eval()
# a_i, interms_i = dn(inp)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="code.yolo.darknet.create_modules"><code class="name flex">
<span>def <span class="ident">create_modules</span></span>(<span>blocks)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_modules(blocks):
    net_info = blocks[0]  # Captures the information about the input and pre-processing

    module_list = nn.ModuleList()

    index = 0  # indexing blocks helps with implementing route  layers (skip connections)

    prev_filters = 3

    output_filters = []

    for x in blocks:
        module = nn.Sequential()

        if (x[&#34;type&#34;] == &#34;net&#34;):
            continue

        # If it&#39;s a convolutional layer
        if (x[&#34;type&#34;] == &#34;convolutional&#34;):
            # Get the info about the layer
            activation = x[&#34;activation&#34;]
            try:
                batch_normalize = int(x[&#34;batch_normalize&#34;])
                bias = False
            except:
                batch_normalize = 0
                bias = True

            filters = int(x[&#34;filters&#34;])
            padding = int(x[&#34;pad&#34;])
            kernel_size = int(x[&#34;size&#34;])
            stride = int(x[&#34;stride&#34;])

            if padding:
                pad = (kernel_size - 1) // 2
            else:
                pad = 0

            # Add the convolutional layer
            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)
            module.add_module(&#34;conv_{0}&#34;.format(index), conv)

            # Add the Batch Norm Layer
            if batch_normalize:
                bn = nn.BatchNorm2d(filters)
                module.add_module(&#34;batch_norm_{0}&#34;.format(index), bn)

            # Check the activation.
            # It is either Linear or a Leaky ReLU for YOLO
            if activation == &#34;leaky&#34;:
                activn = nn.LeakyReLU(0.1, inplace=True)
                module.add_module(&#34;leaky_{0}&#34;.format(index), activn)



        # If it&#39;s an upsampling layer
        # We use Bilinear2dUpsampling

        elif (x[&#34;type&#34;] == &#34;upsample&#34;):
            stride = int(x[&#34;stride&#34;])
            #            upsample = Upsample(stride)
            upsample = nn.Upsample(scale_factor=2, mode=&#34;nearest&#34;)
            module.add_module(&#34;upsample_{}&#34;.format(index), upsample)

        # If it is a route layer
        elif (x[&#34;type&#34;] == &#34;route&#34;):
            x[&#34;layers&#34;] = x[&#34;layers&#34;].split(&#39;,&#39;)

            # Start  of a route
            start = int(x[&#34;layers&#34;][0])

            # end, if there exists one.
            try:
                end = int(x[&#34;layers&#34;][1])
            except:
                end = 0

            # Positive anotation
            if start &gt; 0:
                start = start - index

            if end &gt; 0:
                end = end - index

            route = EmptyLayer()
            module.add_module(&#34;route_{0}&#34;.format(index), route)

            if end &lt; 0:
                filters = output_filters[index + start] + output_filters[index + end]
            else:
                filters = output_filters[index + start]



        # shortcut corresponds to skip connection
        elif x[&#34;type&#34;] == &#34;shortcut&#34;:
            from_ = int(x[&#34;from&#34;])
            shortcut = EmptyLayer()
            module.add_module(&#34;shortcut_{}&#34;.format(index), shortcut)


        elif x[&#34;type&#34;] == &#34;maxpool&#34;:
            stride = int(x[&#34;stride&#34;])
            size = int(x[&#34;size&#34;])
            if stride != 1:
                maxpool = nn.MaxPool2d(size, stride)
            else:
                maxpool = MaxPoolStride1(size)

            module.add_module(&#34;maxpool_{}&#34;.format(index), maxpool)

        # Yolo is the detection layer
        elif x[&#34;type&#34;] == &#34;yolo&#34;:
            mask = x[&#34;mask&#34;].split(&#34;,&#34;)
            mask = [int(x) for x in mask]

            anchors = x[&#34;anchors&#34;].split(&#34;,&#34;)
            anchors = [int(a) for a in anchors]
            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]
            anchors = [anchors[i] for i in mask]

            detection = DetectionLayer(anchors)
            module.add_module(&#34;Detection_{}&#34;.format(index), detection)



        else:
            print(&#34;Something I dunno&#34;)
            assert False

        module_list.append(module)
        prev_filters = filters
        output_filters.append(filters)
        index += 1

    return (net_info, module_list)</code></pre>
</details>
</dd>
<dt id="code.yolo.darknet.get_test_input"><code class="name flex">
<span>def <span class="ident">get_test_input</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_test_input():
    img = cv2.imread(&#34;dog-cycle-car.png&#34;)
    img = cv2.resize(img, (416, 416))
    img_ = img[:, :, ::-1].transpose((2, 0, 1))
    img_ = img_[np.newaxis, :, :, :] / 255.0
    img_ = torch.from_numpy(img_).float()
    img_ = Variable(img_)
    return img_</code></pre>
</details>
</dd>
<dt id="code.yolo.darknet.parse_cfg"><code class="name flex">
<span>def <span class="ident">parse_cfg</span></span>(<span>cfgfile)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes a configuration file</p>
<p>Returns a list of blocks. Each blocks describes a block in the neural
network to be built. Block is represented as a dictionary in the list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_cfg(cfgfile):
    &#34;&#34;&#34;
    Takes a configuration file
    
    Returns a list of blocks. Each blocks describes a block in the neural
    network to be built. Block is represented as a dictionary in the list
    
    &#34;&#34;&#34;
    file = open(cfgfile, &#39;r&#39;)
    lines = file.read().split(&#39;\n&#39;)  # store the lines in a list
    lines = [x for x in lines if len(x) &gt; 0]  # get read of the empty lines
    lines = [x for x in lines if x[0] != &#39;#&#39;]
    lines = [x.rstrip().lstrip() for x in lines]

    block = {}
    blocks = []

    for line in lines:
        if line[0] == &#34;[&#34;:  # This marks the start of a new block
            if len(block) != 0:
                blocks.append(block)
                block = {}
            block[&#34;type&#34;] = line[1:-1].rstrip()
        else:
            key, value = line.split(&#34;=&#34;)
            block[key.rstrip()] = value.lstrip()
    blocks.append(block)

    return blocks</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="code.yolo.darknet.Darknet"><code class="flex name class">
<span>class <span class="ident">Darknet</span></span>
<span>(</span><span>cfgfile)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Darknet(nn.Module):
    def __init__(self, cfgfile):
        super(Darknet, self).__init__()
        self.blocks = parse_cfg(cfgfile)
        self.net_info, self.module_list = create_modules(self.blocks)
        self.header = torch.IntTensor([0, 0, 0, 0])
        self.seen = 0

    def get_blocks(self):
        return self.blocks

    def get_module_list(self):
        return self.module_list

    def forward(self, x, CUDA):
        detections = []
        modules = self.blocks[1:]
        outputs = {}  # We cache the outputs for the route layer

        write = 0
        for i in range(len(modules)):

            module_type = (modules[i][&#34;type&#34;])
            if module_type == &#34;convolutional&#34; or module_type == &#34;upsample&#34; or module_type == &#34;maxpool&#34;:

                x = self.module_list[i](x)
                outputs[i] = x


            elif module_type == &#34;route&#34;:
                layers = modules[i][&#34;layers&#34;]
                layers = [int(a) for a in layers]

                if (layers[0]) &gt; 0:
                    layers[0] = layers[0] - i

                if len(layers) == 1:
                    x = outputs[i + (layers[0])]

                else:
                    if (layers[1]) &gt; 0:
                        layers[1] = layers[1] - i

                    map1 = outputs[i + layers[0]]
                    map2 = outputs[i + layers[1]]

                    x = torch.cat((map1, map2), 1)
                outputs[i] = x

            elif module_type == &#34;shortcut&#34;:
                from_ = int(modules[i][&#34;from&#34;])
                x = outputs[i - 1] + outputs[i + from_]
                outputs[i] = x



            elif module_type == &#39;yolo&#39;:

                anchors = self.module_list[i][0].anchors
                # Get the input dimensions
                inp_dim = int(self.net_info[&#34;height&#34;])

                # Get the number of classes
                num_classes = int(modules[i][&#34;classes&#34;])

                # Output the result
                x = x.data
                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)

                if type(x) == int:
                    continue

                if not write:
                    detections = x
                    write = 1

                else:
                    detections = torch.cat((detections, x), 1)

                outputs[i] = outputs[i - 1]

        try:
            return detections
        except:
            return 0

    def load_weights(self, weightfile):

        # Open the weights file
        fp = open(weightfile, &#34;rb&#34;)

        # The first 4 values are header information
        # 1. Major version number
        # 2. Minor Version Number
        # 3. Subversion number 
        # 4. IMages seen 
        header = np.fromfile(fp, dtype=np.int32, count=5)
        self.header = torch.from_numpy(header)
        self.seen = self.header[3]

        # The rest of the values are the weights
        # Let&#39;s load them up
        weights = np.fromfile(fp, dtype=np.float32)

        ptr = 0
        for i in range(len(self.module_list)):
            module_type = self.blocks[i + 1][&#34;type&#34;]

            if module_type == &#34;convolutional&#34;:
                model = self.module_list[i]
                try:
                    batch_normalize = int(self.blocks[i + 1][&#34;batch_normalize&#34;])
                except:
                    batch_normalize = 0

                conv = model[0]

                if (batch_normalize):
                    bn = model[1]

                    # Get the number of weights of Batch Norm Layer
                    num_bn_biases = bn.bias.numel()

                    # Load the weights
                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])
                    ptr += num_bn_biases

                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                    ptr += num_bn_biases

                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                    ptr += num_bn_biases

                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                    ptr += num_bn_biases

                    # Cast the loaded weights into dims of models weights.
                    bn_biases = bn_biases.view_as(bn.bias.data)
                    bn_weights = bn_weights.view_as(bn.weight.data)
                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)
                    bn_running_var = bn_running_var.view_as(bn.running_var)

                    # Copy the data to models
                    bn.bias.data.copy_(bn_biases)
                    bn.weight.data.copy_(bn_weights)
                    bn.running_mean.copy_(bn_running_mean)
                    bn.running_var.copy_(bn_running_var)

                else:
                    # Number of biases
                    num_biases = conv.bias.numel()

                    # Load the weights
                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])
                    ptr = ptr + num_biases

                    # reshape the loaded weights according to the dims of the models weights
                    conv_biases = conv_biases.view_as(conv.bias.data)

                    # Finally copy the data
                    conv.bias.data.copy_(conv_biases)

                # Let us load the weights for the Convolutional layers
                num_weights = conv.weight.numel()

                # Do the same as above for weights
                conv_weights = torch.from_numpy(weights[ptr:ptr + num_weights])
                ptr = ptr + num_weights

                conv_weights = conv_weights.view_as(conv.weight.data)
                conv.weight.data.copy_(conv_weights)

    def save_weights(self, savedfile, cutoff=0):

        if cutoff &lt;= 0:
            cutoff = len(self.blocks) - 1

        fp = open(savedfile, &#39;wb&#39;)

        # Attach the header at the top of the file
        self.header[3] = self.seen
        header = self.header

        header = header.numpy()
        header.tofile(fp)

        # Now, let us save the weights 
        for i in range(len(self.module_list)):
            module_type = self.blocks[i + 1][&#34;type&#34;]

            if (module_type) == &#34;convolutional&#34;:
                model = self.module_list[i]
                try:
                    batch_normalize = int(self.blocks[i + 1][&#34;batch_normalize&#34;])
                except:
                    batch_normalize = 0

                conv = model[0]

                if (batch_normalize):
                    bn = model[1]

                    # If the parameters are on GPU, convert them back to CPU
                    # We don&#39;t convert the parameter to GPU
                    # Instead. we copy the parameter and then convert it to CPU
                    # This is done as weight are need to be saved during training
                    cpu(bn.bias.data).numpy().tofile(fp)
                    cpu(bn.weight.data).numpy().tofile(fp)
                    cpu(bn.running_mean).numpy().tofile(fp)
                    cpu(bn.running_var).numpy().tofile(fp)


                else:
                    cpu(conv.bias.data).numpy().tofile(fp)

                # Let us save the weights for the Convolutional layers
                cpu(conv.weight.data).numpy().tofile(fp)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="code.yolo.darknet.Darknet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, CUDA)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, CUDA):
    detections = []
    modules = self.blocks[1:]
    outputs = {}  # We cache the outputs for the route layer

    write = 0
    for i in range(len(modules)):

        module_type = (modules[i][&#34;type&#34;])
        if module_type == &#34;convolutional&#34; or module_type == &#34;upsample&#34; or module_type == &#34;maxpool&#34;:

            x = self.module_list[i](x)
            outputs[i] = x


        elif module_type == &#34;route&#34;:
            layers = modules[i][&#34;layers&#34;]
            layers = [int(a) for a in layers]

            if (layers[0]) &gt; 0:
                layers[0] = layers[0] - i

            if len(layers) == 1:
                x = outputs[i + (layers[0])]

            else:
                if (layers[1]) &gt; 0:
                    layers[1] = layers[1] - i

                map1 = outputs[i + layers[0]]
                map2 = outputs[i + layers[1]]

                x = torch.cat((map1, map2), 1)
            outputs[i] = x

        elif module_type == &#34;shortcut&#34;:
            from_ = int(modules[i][&#34;from&#34;])
            x = outputs[i - 1] + outputs[i + from_]
            outputs[i] = x



        elif module_type == &#39;yolo&#39;:

            anchors = self.module_list[i][0].anchors
            # Get the input dimensions
            inp_dim = int(self.net_info[&#34;height&#34;])

            # Get the number of classes
            num_classes = int(modules[i][&#34;classes&#34;])

            # Output the result
            x = x.data
            x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)

            if type(x) == int:
                continue

            if not write:
                detections = x
                write = 1

            else:
                detections = torch.cat((detections, x), 1)

            outputs[i] = outputs[i - 1]

    try:
        return detections
    except:
        return 0</code></pre>
</details>
</dd>
<dt id="code.yolo.darknet.Darknet.get_blocks"><code class="name flex">
<span>def <span class="ident">get_blocks</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_blocks(self):
    return self.blocks</code></pre>
</details>
</dd>
<dt id="code.yolo.darknet.Darknet.get_module_list"><code class="name flex">
<span>def <span class="ident">get_module_list</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_module_list(self):
    return self.module_list</code></pre>
</details>
</dd>
<dt id="code.yolo.darknet.Darknet.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weightfile)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weightfile):

    # Open the weights file
    fp = open(weightfile, &#34;rb&#34;)

    # The first 4 values are header information
    # 1. Major version number
    # 2. Minor Version Number
    # 3. Subversion number 
    # 4. IMages seen 
    header = np.fromfile(fp, dtype=np.int32, count=5)
    self.header = torch.from_numpy(header)
    self.seen = self.header[3]

    # The rest of the values are the weights
    # Let&#39;s load them up
    weights = np.fromfile(fp, dtype=np.float32)

    ptr = 0
    for i in range(len(self.module_list)):
        module_type = self.blocks[i + 1][&#34;type&#34;]

        if module_type == &#34;convolutional&#34;:
            model = self.module_list[i]
            try:
                batch_normalize = int(self.blocks[i + 1][&#34;batch_normalize&#34;])
            except:
                batch_normalize = 0

            conv = model[0]

            if (batch_normalize):
                bn = model[1]

                # Get the number of weights of Batch Norm Layer
                num_bn_biases = bn.bias.numel()

                # Load the weights
                bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])
                ptr += num_bn_biases

                bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                ptr += num_bn_biases

                bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                ptr += num_bn_biases

                bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
                ptr += num_bn_biases

                # Cast the loaded weights into dims of models weights.
                bn_biases = bn_biases.view_as(bn.bias.data)
                bn_weights = bn_weights.view_as(bn.weight.data)
                bn_running_mean = bn_running_mean.view_as(bn.running_mean)
                bn_running_var = bn_running_var.view_as(bn.running_var)

                # Copy the data to models
                bn.bias.data.copy_(bn_biases)
                bn.weight.data.copy_(bn_weights)
                bn.running_mean.copy_(bn_running_mean)
                bn.running_var.copy_(bn_running_var)

            else:
                # Number of biases
                num_biases = conv.bias.numel()

                # Load the weights
                conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])
                ptr = ptr + num_biases

                # reshape the loaded weights according to the dims of the models weights
                conv_biases = conv_biases.view_as(conv.bias.data)

                # Finally copy the data
                conv.bias.data.copy_(conv_biases)

            # Let us load the weights for the Convolutional layers
            num_weights = conv.weight.numel()

            # Do the same as above for weights
            conv_weights = torch.from_numpy(weights[ptr:ptr + num_weights])
            ptr = ptr + num_weights

            conv_weights = conv_weights.view_as(conv.weight.data)
            conv.weight.data.copy_(conv_weights)</code></pre>
</details>
</dd>
<dt id="code.yolo.darknet.Darknet.save_weights"><code class="name flex">
<span>def <span class="ident">save_weights</span></span>(<span>self, savedfile, cutoff=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_weights(self, savedfile, cutoff=0):

    if cutoff &lt;= 0:
        cutoff = len(self.blocks) - 1

    fp = open(savedfile, &#39;wb&#39;)

    # Attach the header at the top of the file
    self.header[3] = self.seen
    header = self.header

    header = header.numpy()
    header.tofile(fp)

    # Now, let us save the weights 
    for i in range(len(self.module_list)):
        module_type = self.blocks[i + 1][&#34;type&#34;]

        if (module_type) == &#34;convolutional&#34;:
            model = self.module_list[i]
            try:
                batch_normalize = int(self.blocks[i + 1][&#34;batch_normalize&#34;])
            except:
                batch_normalize = 0

            conv = model[0]

            if (batch_normalize):
                bn = model[1]

                # If the parameters are on GPU, convert them back to CPU
                # We don&#39;t convert the parameter to GPU
                # Instead. we copy the parameter and then convert it to CPU
                # This is done as weight are need to be saved during training
                cpu(bn.bias.data).numpy().tofile(fp)
                cpu(bn.weight.data).numpy().tofile(fp)
                cpu(bn.running_mean).numpy().tofile(fp)
                cpu(bn.running_var).numpy().tofile(fp)


            else:
                cpu(conv.bias.data).numpy().tofile(fp)

            # Let us save the weights for the Convolutional layers
            cpu(conv.weight.data).numpy().tofile(fp)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="code.yolo.darknet.DetectionLayer"><code class="flex name class">
<span>class <span class="ident">DetectionLayer</span></span>
<span>(</span><span>anchors)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DetectionLayer(nn.Module):
    def __init__(self, anchors):
        super(DetectionLayer, self).__init__()
        self.anchors = anchors

    def forward(self, x, inp_dim, num_classes, confidence):
        x = x.data
        global CUDA
        prediction = x
        prediction = predict_transform(prediction, inp_dim, self.anchors, num_classes, confidence, CUDA)
        return prediction</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="code.yolo.darknet.DetectionLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, inp_dim, num_classes, confidence)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, inp_dim, num_classes, confidence):
    x = x.data
    global CUDA
    prediction = x
    prediction = predict_transform(prediction, inp_dim, self.anchors, num_classes, confidence, CUDA)
    return prediction</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="code.yolo.darknet.EmptyLayer"><code class="flex name class">
<span>class <span class="ident">EmptyLayer</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmptyLayer(nn.Module):
    def __init__(self):
        super(EmptyLayer, self).__init__()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
</dd>
<dt id="code.yolo.darknet.MaxPoolStride1"><code class="flex name class">
<span>class <span class="ident">MaxPoolStride1</span></span>
<span>(</span><span>kernel_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxPoolStride1(nn.Module):
    def __init__(self, kernel_size):
        super(MaxPoolStride1, self).__init__()
        self.kernel_size = kernel_size
        self.pad = kernel_size - 1

    def forward(self, x):
        padded_x = F.pad(x, (0, self.pad, 0, self.pad), mode=&#34;replicate&#34;)
        pooled_x = nn.MaxPool2d(self.kernel_size, self.pad)(padded_x)
        return pooled_x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="code.yolo.darknet.MaxPoolStride1.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    padded_x = F.pad(x, (0, self.pad, 0, self.pad), mode=&#34;replicate&#34;)
    pooled_x = nn.MaxPool2d(self.kernel_size, self.pad)(padded_x)
    return pooled_x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="code.yolo.darknet.ReOrgLayer"><code class="flex name class">
<span>class <span class="ident">ReOrgLayer</span></span>
<span>(</span><span>stride=2)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReOrgLayer(nn.Module):
    def __init__(self, stride=2):
        super(ReOrgLayer, self).__init__()
        self.stride = stride

    def forward(self, x):
        assert (x.data.dim() == 4)
        B, C, H, W = x.data.shape
        hs = self.stride
        ws = self.stride
        assert (H % hs == 0), &#34;The stride &#34; + str(self.stride) + &#34; is not a proper divisor of height &#34; + str(H)
        assert (W % ws == 0), &#34;The stride &#34; + str(self.stride) + &#34; is not a proper divisor of height &#34; + str(W)
        x = x.view(B, C, H // hs, hs, W // ws, ws).transpose(-2, -3).contiguous()
        x = x.view(B, C, H // hs * W // ws, hs, ws)
        x = x.view(B, C, H // hs * W // ws, hs * ws).transpose(-1, -2).contiguous()
        x = x.view(B, C, ws * hs, H // ws, W // ws).transpose(1, 2).contiguous()
        x = x.view(B, C * ws * hs, H // ws, W // ws)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="code.yolo.darknet.ReOrgLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    assert (x.data.dim() == 4)
    B, C, H, W = x.data.shape
    hs = self.stride
    ws = self.stride
    assert (H % hs == 0), &#34;The stride &#34; + str(self.stride) + &#34; is not a proper divisor of height &#34; + str(H)
    assert (W % ws == 0), &#34;The stride &#34; + str(self.stride) + &#34; is not a proper divisor of height &#34; + str(W)
    x = x.view(B, C, H // hs, hs, W // ws, ws).transpose(-2, -3).contiguous()
    x = x.view(B, C, H // hs * W // ws, hs, ws)
    x = x.view(B, C, H // hs * W // ws, hs * ws).transpose(-1, -2).contiguous()
    x = x.view(B, C, ws * hs, H // ws, W // ws).transpose(1, 2).contiguous()
    x = x.view(B, C * ws * hs, H // ws, W // ws)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="code.yolo.darknet.Upsample"><code class="flex name class">
<span>class <span class="ident">Upsample</span></span>
<span>(</span><span>stride=2)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Upsample(nn.Module):
    def __init__(self, stride=2):
        super(Upsample, self).__init__()
        self.stride = stride

    def forward(self, x):
        stride = self.stride
        assert (x.data.dim() == 4)
        B = x.data.size(0)
        C = x.data.size(1)
        H = x.data.size(2)
        W = x.data.size(3)
        ws = stride
        hs = stride
        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H * stride, W * stride)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="code.yolo.darknet.Upsample.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    stride = self.stride
    assert (x.data.dim() == 4)
    B = x.data.size(0)
    C = x.data.size(1)
    H = x.data.size(2)
    W = x.data.size(3)
    ws = stride
    hs = stride
    x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H * stride, W * stride)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="code.yolo.darknet.test_net"><code class="flex name class">
<span>class <span class="ident">test_net</span></span>
<span>(</span><span>num_layers, input_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class test_net(nn.Module):
    def __init__(self, num_layers, input_size):
        super(test_net, self).__init__()
        self.num_layers = num_layers
        self.linear_1 = nn.Linear(input_size, 5)
        self.middle = nn.ModuleList([nn.Linear(5, 5) for x in range(num_layers)])
        self.output = nn.Linear(5, 2)

    def forward(self, x):
        x = x.view(-1)
        fwd = nn.Sequential(self.linear_1, *self.middle, self.output)
        return fwd(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="code.yolo.darknet.test_net.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = x.view(-1)
    fwd = nn.Sequential(self.linear_1, *self.middle, self.output)
    return fwd(x)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="code.yolo" href="index.html">code.yolo</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="code.yolo.darknet.create_modules" href="#code.yolo.darknet.create_modules">create_modules</a></code></li>
<li><code><a title="code.yolo.darknet.get_test_input" href="#code.yolo.darknet.get_test_input">get_test_input</a></code></li>
<li><code><a title="code.yolo.darknet.parse_cfg" href="#code.yolo.darknet.parse_cfg">parse_cfg</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="code.yolo.darknet.Darknet" href="#code.yolo.darknet.Darknet">Darknet</a></code></h4>
<ul class="">
<li><code><a title="code.yolo.darknet.Darknet.forward" href="#code.yolo.darknet.Darknet.forward">forward</a></code></li>
<li><code><a title="code.yolo.darknet.Darknet.get_blocks" href="#code.yolo.darknet.Darknet.get_blocks">get_blocks</a></code></li>
<li><code><a title="code.yolo.darknet.Darknet.get_module_list" href="#code.yolo.darknet.Darknet.get_module_list">get_module_list</a></code></li>
<li><code><a title="code.yolo.darknet.Darknet.load_weights" href="#code.yolo.darknet.Darknet.load_weights">load_weights</a></code></li>
<li><code><a title="code.yolo.darknet.Darknet.save_weights" href="#code.yolo.darknet.Darknet.save_weights">save_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="code.yolo.darknet.DetectionLayer" href="#code.yolo.darknet.DetectionLayer">DetectionLayer</a></code></h4>
<ul class="">
<li><code><a title="code.yolo.darknet.DetectionLayer.forward" href="#code.yolo.darknet.DetectionLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="code.yolo.darknet.EmptyLayer" href="#code.yolo.darknet.EmptyLayer">EmptyLayer</a></code></h4>
</li>
<li>
<h4><code><a title="code.yolo.darknet.MaxPoolStride1" href="#code.yolo.darknet.MaxPoolStride1">MaxPoolStride1</a></code></h4>
<ul class="">
<li><code><a title="code.yolo.darknet.MaxPoolStride1.forward" href="#code.yolo.darknet.MaxPoolStride1.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="code.yolo.darknet.ReOrgLayer" href="#code.yolo.darknet.ReOrgLayer">ReOrgLayer</a></code></h4>
<ul class="">
<li><code><a title="code.yolo.darknet.ReOrgLayer.forward" href="#code.yolo.darknet.ReOrgLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="code.yolo.darknet.Upsample" href="#code.yolo.darknet.Upsample">Upsample</a></code></h4>
<ul class="">
<li><code><a title="code.yolo.darknet.Upsample.forward" href="#code.yolo.darknet.Upsample.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="code.yolo.darknet.test_net" href="#code.yolo.darknet.test_net">test_net</a></code></h4>
<ul class="">
<li><code><a title="code.yolo.darknet.test_net.forward" href="#code.yolo.darknet.test_net.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.3</a>.</p>
</footer>
</body>
</html>