<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.3" />
<title>code.tasks.image_processing API documentation</title>
<meta name="description" content="This module contains generic image processing functions that can also be used
for applications other than Painting Detection." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>code.tasks.image_processing</code></h1>
</header>
<section id="section-intro">
<p>This module contains generic image processing functions that can also be used
for applications other than Painting Detection.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module contains generic image processing functions that can also be used
for applications other than Painting Detection.
&#34;&#34;&#34;

import cv2
import numpy as np


def automatic_brightness_and_contrast(image, clip_hist_percent=1):
    &#34;&#34;&#34;Adjust automatically brightness and contrast of the image.

    Brightness and contrast is linear operator with parameter alpha and beta:
        g(x,y)= α * f(x,y)+ β

    It is recommended to visit the first link in the notes.

    Parameters
    ----------
    img: ndarray
        the input image

    Returns
    -------
    tuple
        new_img = adjusted image,
        alpha = alpha value calculated,
        beta = beta value calculated

    Notes
    -----
    For details visit.
    - https://stackoverflow.com/questions/56905592/automatic-contrast-and-brightness-adjustment-of-a-color-photo-of-a-sheet-of-pape
    - https://answers.opencv.org/question/75510/how-to-make-auto-adjustmentsbrightness-and-contrast-for-image-android-opencv-image-correction/
    - https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#convertscaleabs
    &#34;&#34;&#34;
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Calculate grayscale histogram
    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
    hist_size = len(hist)

    # Calculate cumulative distribution from the histogram
    accumulator = []
    accumulator.append(float(hist[0]))
    for index in range(1, hist_size):
        accumulator.append(accumulator[index - 1] + float(hist[index]))

    # Locate points to clip
    maximum = accumulator[-1]
    clip_hist_percent *= (maximum / 100.0)
    clip_hist_percent /= 2.0

    # Locate left cut
    minimum_gray = 0
    while accumulator[minimum_gray] &lt; clip_hist_percent:
        minimum_gray += 1

    # Locate right cut
    maximum_gray = hist_size - 1
    while accumulator[maximum_gray] &gt;= (maximum - clip_hist_percent):
        maximum_gray -= 1

    # Calculate alpha and beta values
    alpha = 255 / (maximum_gray - minimum_gray)
    beta = -minimum_gray * alpha

    &#39;&#39;&#39;
    # Calculate new histogram with desired range and show histogram 
    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])
    plt.plot(hist)
    plt.plot(new_hist)
    plt.xlim([0,256])
    plt.show()
    &#39;&#39;&#39;

    new_img = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)
    return new_img, alpha, beta


def image_dilation(img, kernel_size):
    &#34;&#34;&#34;Dilate the image.

    Dilation involves moving a kernel over the pixels of a binary image. When
    the kernel is centered on a pixel with a value of 0 and some of its pixels
    are on pixels with a value of 1, the centre pixel is given a value of 1.

    Parameters
    ----------
    img: ndarray
        the img to be dilated
    kernel_size: int
        the kernel size
    kernel_value: int
        the kernel value (the value of each kernel pixel)

    Returns
    -------
    ndarray
        Returns the dilated img
    &#34;&#34;&#34;

    kernel = np.ones((kernel_size, kernel_size))
    dilated_img = cv2.dilate(img, kernel)
    return dilated_img


def image_erosion(img, kernel_size):
    &#34;&#34;&#34;Erode the image.

    It&#39;s the opposite of dilation. Erosion involves moving a kernel over the
    pixels of a binary image. A pixel in the original image (either 1 or 0)
    will be considered 1 only if all the pixels under the kernel is 1,
    otherwise it is eroded (made to zero).

    Parameters
    ----------
    img: ndarray
        the img to be eroded
    kernel_size: int
        the kernel size
    kernel_value: int
        the kernel value (the value of each kernel pixel)

    Returns
    -------
    ndarray
        Returns the eroded image

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/trunk/d9/d61/tutorial_py_morphological_ops.html
    &#34;&#34;&#34;

    kernel = np.ones((kernel_size, kernel_size))
    eroded_img = cv2.erode(img, kernel)
    return eroded_img


def image_morphology_tranformation(img, operation, kernel_size):
    &#34;&#34;&#34;Performs morphological transformations

    Performs morphological transformations of the image using an erosion
    and dilation.

    Parameters
    ----------
    img: ndarray
        the input image
    operation: int
        type of operation
    kernel_size: int
        the kernel size

    Returns
    -------
    ndarray
        the transformed image of the same size and type as source image

    &#34;&#34;&#34;
    kernel = np.ones((kernel_size, kernel_size))
    transformed_img = cv2.morphologyEx(img, operation, kernel)
    return transformed_img


def image_blurring(img, ksize):
    &#34;&#34;&#34;Blurs an image using the median filter.

    Parameters
    ----------
    img: ndarray
        the input image
    ksize: int
        aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...

    Returns
    -------
    ndarray
        the image blurred

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9
    - https://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html
    &#34;&#34;&#34;

    assert ksize &gt; 1 and ksize % 2 != 0, &#34;`ksize` should be odd and grater than 1&#34;
    return cv2.medianBlur(img, ksize)


def invert_image(img):
    &#34;&#34;&#34;Returns an inverted version of the image.

    The function calculates per-element bit-wise inversion of the input image.
    This means that black (=0) pixels become white (=255), and vice versa.

    In our case, we need to invert the wall mask for finding possible painting components.

    From OpenCV documentation (https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html):
    &#34;
        In OpenCV, finding contours is like finding white object from black
        background. So remember, object to be found should be white and background
        should be black.
    &#34;

    Parameters
    ----------
    img: ndarray
        the image to invert

    Returns
    -------
    ndarray
        the inverted image
    &#34;&#34;&#34;

    return cv2.bitwise_not(img)


def find_image_contours(img, mode, method):
    &#34;&#34;&#34;Finds contours in a binary image.

    The function retrieves contours from a binary image (i.e. the wall mask we
    find before)

    Parameters
    ----------
    img: ndarray
        binary image in which to find the contours (i.e. the wall mask)
    mode: int
        Contour retrieval mode
    method: int
        Contour approximation method

    Returns
    -------
    contours: list
        Detected contours. Each contour is stored as a list of all the
        contours in the image. Each individual contour is a Numpy array
        of (x,y) coordinates of boundary points of the object.

    Notes
    -----
    Fot details visit:
    - https://docs.opencv.org/trunk/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0
    - https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html
    &#34;&#34;&#34;

    contours, hierarchy = cv2.findContours(img, mode, method)

    return contours, hierarchy


#
def canny_edge_detection(img, threshold1, threshold2):
    &#34;&#34;&#34;Finds edges in an image using the Canny algorithm.

    Parameters
    ----------
    img: ndarray
        the input image
    threshold1: int
        first threshold for the hysteresis procedure.
    threshold2: int
        second threshold for the hysteresis procedure.

    Returns
    -------
    ndarray
        returns an edge map that has the same size and type as `img`

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny#canny
    - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html
    &#34;&#34;&#34;
    return cv2.Canny(img, threshold1, threshold2)


def find_hough_lines(img, probabilistic_mode=False, rho=1, theta=np.pi / 180, threshold=0, ratio_percentage=0.15):
    &#34;&#34;&#34;Detect straight lines.

    Detect straight lines using the Standard or Probabilistic Hough
    Line Transform.

    Parameters
    ----------
    img: ndarray
        input image
    probabilistic_mode: bool
        determines whether to use the Standard (False) or the Probabilistic
        (True) Hough Transform
    rho: int
        distance resolution of the accumulator in pixels.
    theta: float
        angle resolution of the accumulator in radians.
    threshold: int
        accumulator threshold parameter. Only those rows that get enough
        votes ( &gt;`threshold` ) are returned.
    ratio_percentage: float
        percentage of the image&#39;s larger side. The image is searched for
        lines who&#39;s length is at least a certain percentage of the image&#39;s
        larger side (default 15%).

    Returns
    -------
    ndarray
        Returns a NumPy.array of lines

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html
    - https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a
    &#34;&#34;&#34;

    h, w = img.shape
    if probabilistic_mode:
        img_ratio = np.max([h, w]) * ratio_percentage
        lines = cv2.HoughLinesP(img, rho, theta, threshold, img_ratio, img_ratio / 3.5)
    else:
        lines = cv2.HoughLines(img, rho, theta, threshold, None, 0, 0)

    return lines


def extend_image_lines(img, lines, probabilistic_mode, color_value=255):
    &#34;&#34;&#34;Create a mask by extending the lines received.

    Create a mask of the same size of the image `img`, where the `lines` received
    have been drawn in order to cross the whole image. The color used to draw
    the lines is specified by `color_value`.

    Parameters
    ----------
    img: ndarray
        the input image
    lines: ndarray
        a NumPy.array of lines
    probabilistic_mode: bool
        determines whether to use the Standard (False) or the Probabilistic
        (True) Hough Transform
    color_value: tuple
        tuple (B,G,R) that specifies the color of the lines

    Returns
    -------
    ndarray
        Returns the mask with the lines drawn.

    Notes
    -----
    For details visit:
    - https://answers.opencv.org/question/2966/how-do-the-rho-and-theta-values-work-in-houghlines/#:~:text=rho%20is%20the%20distance%20from,are%20called%20rho%20and%20theta.
    &#34;&#34;&#34;

    h = img.shape[0]
    w = img.shape[1]

    mask = np.zeros((h, w), dtype=np.uint8)

    length = np.max((h, w))

    for line in lines:
        line = line[0]
        if probabilistic_mode:
            theta = np.arctan2(line[1] - line[3], line[0] - line[2])

            x0 = line[0]
            y0 = line[1]

            a = np.cos(theta)
            b = np.sin(theta)

            pt1 = (int(x0 - length * a), int(y0 - length * b),)
            pt2 = (int(x0 + length * a), int(y0 + length * b),)
        else:
            rho = line[0]
            theta = line[1]

            a = np.cos(theta)
            b = np.sin(theta)

            # Read: https://answers.opencv.org/question/2966/how-do-the-rho-and-theta-values-work-in-houghlines/#:~:text=rho%20is%20the%20distance%20from,are%20called%20rho%20and%20theta.
            x0 = a * rho
            y0 = b * rho

            length = 40000

            pt1 = (int(x0 + length * (-b)), int(y0 + length * (a)))
            pt2 = (int(x0 - length * (-b)), int(y0 - length * (a)))

        cv2.line(mask, pt1, pt2, color_value, 2, cv2.LINE_AA)  # cv2.LINE_AA

    return mask


def find_corners(img, max_number_corners=4, corner_quality=0.001, min_distance=20):
    &#34;&#34;&#34;Perform Shi-Tomasi Corner detection.

    Perform Shi-Tomasi Corner detection to return the corners found in the image.

    Parameters
    ----------
    img: ndarray
        the input image
    max_number_corners: int
        maximum number of corners to return
    corner_quality: float
        minimal accepted quality of image corners. The corners with the quality
        measure less than the product are rejected.
    min_distance: int
        minimum Euclidean distance between the returned corners

    Returns
    -------
    ndarray
        Returns a NumPy array of the most prominent corners in the image, in the
        form (x,y).

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541
    - https://docs.opencv.org/master/d4/d8c/tutorial_py_shi_tomasi.html
    - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html
    &#34;&#34;&#34;

    corners = cv2.goodFeaturesToTrack(
        img,
        max_number_corners,
        corner_quality,
        min_distance
    )

    return corners


def mean_shift_segmentation(img, spatial_radius, color_radius, maximum_pyramid_level):
    &#34;&#34;&#34;Groups pixels together by colour and location.

    This function takes an image and mean-shift parameters and returns a version
    of the image that has had mean shift segmentation performed on it.

    Mean shift segmentation clusters nearby pixels with similar pixel values and
    sets them all to have the value of the local maxima of pixel value.

    Parameters
    ----------
    img: ndarray
        image to apply the Mean Shift Segmentation
    spatial_radius: int
        The spatial window radius
    color_radius: int
        The color window radius
    maximum_pyramid_level: int
        Maximum level of the pyramid for the segmentation

    Returns
    -------
    ndarray
        filtered “posterized” image with color gradients and fine-grain
        texture flattened

    Notes
    -------
    For details visit:
    - https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering
    - https://docs.opencv.org/master/d7/d00/tutorial_meanshift.html

    &#34;&#34;&#34;

    dst_img = cv2.pyrMeanShiftFiltering(img, spatial_radius, color_radius, maximum_pyramid_level)
    return dst_img


def find_largest_segment(img, color_difference=1, x_samples=8):
    &#34;&#34;&#34;Create a mask using the largest segment (this segment will be white).

    This is done by setting every pixel that is not the same color of the wall
    to have a value of 0 and every pixel has a value within a euclidean distance
    of `color_difference` to the wall&#39;s pixel value to have a value of 255.

    Parameters
    ----------
    img: ndarray
        image to apply masking
    color_difference: int
        euclidean distance between wall&#39;s pixel and the rest of the image
    x_samples: int
        numer of samples that will be tested orizontally in the image

    Returns
    -------
    ndarray
        Returns a version of the image where the wall is white and the rest of
        the image is black.
    &#34;&#34;&#34;

    h, w, chn = img.shape
    color_difference = (color_difference,) * 3

    # in that way for smaller images the stride will be lower
    stride = int(w / x_samples)

    mask = np.zeros((h + 2, w + 2), dtype=np.uint8)
    wall_mask = mask
    largest_segment = 0
    for y in range(0, h, stride):
        for x in range(0, w, stride):
            if mask[y + 1, x + 1] == 0:
                mask[:] = 0
                # Fills a connected component with the given color.
                # For details visit:
                # https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html?highlight=floodfill#floodfill
                rect = cv2.floodFill(
                    image=img.copy(),
                    mask=mask,
                    seedPoint=(x, y),
                    newVal=0,
                    loDiff=color_difference,
                    upDiff=color_difference,
                    flags=4 | (255 &lt;&lt; 8),
                )

                # For details visit:
                # https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57

                # Next operation is not necessary if flag is equal to `4 | ( 255 &lt;&lt; 8 )`
                # _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
                segment_size = mask.sum()
                if segment_size &gt; largest_segment:
                    largest_segment = segment_size
                    wall_mask = mask[1:-1, 1:-1].copy()
    #                     show_image(&#39;rect[2]&#39;, mask, height=405, width=720)
    # cv2.waitKey(0)
    return wall_mask


def create_segmented_image(img, contours):
    &#34;&#34;&#34;
    Create an image the the contours are white filled and the rest is black.

    Parameters
    ----------
    img: ndarray
        input image necessary to the shape
    contours: list
        list of contours. Each individual contour is a Numpy array
        of (x,y) coordinates of boundary points of the object.

    Returns
    -------
    ndarray
        the segmented image
    &#34;&#34;&#34;

    h = img.shape[0]
    w = img.shape[1]

    segmented = np.zeros((h, w), dtype=np.uint8)

    cv2.drawContours(segmented, contours, -1, 255, cv2.FILLED)

    return segmented


def image_resize(img, width, height, interpolation=cv2.INTER_CUBIC):
    &#34;&#34;&#34;Resize the input image to the given size.

    Resize the input image to the given size using the given interpolation
    method.

    Parameters
    ----------
    img: ndarray
        the input image
    width: int
        width of the target resized image
    height: int
        height of the target resized image
    interpolation: int
        interpolation method

    Returns
    -------
    tuple
        (ndarray, float) = (the resized image, the scale factor)
    &#34;&#34;&#34;

    scale_factor = 1.
    resized_img = img
    h_img, w_img, c_img = img.shape

    if h_img &gt; height and w_img &gt; width:
        scale_factor = h_img / height
        height_scaled = height
        width_scaled = width
        resized_img = cv2.resize(img, (width_scaled, height_scaled), interpolation)

    return resized_img, scale_factor</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="code.tasks.image_processing.automatic_brightness_and_contrast"><code class="name flex">
<span>def <span class="ident">automatic_brightness_and_contrast</span></span>(<span>image, clip_hist_percent=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Adjust automatically brightness and contrast of the image.</p>
<p>Brightness and contrast is linear operator with parameter alpha and beta:
g(x,y)= α * f(x,y)+ β</p>
<p>It is recommended to visit the first link in the notes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the input image</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>new_img = adjusted image,
alpha = alpha value calculated,
beta = beta value calculated</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit.
- <a href="https://stackoverflow.com/questions/56905592/automatic-contrast-and-brightness-adjustment-of-a-color-photo-of-a-sheet-of-pape">https://stackoverflow.com/questions/56905592/automatic-contrast-and-brightness-adjustment-of-a-color-photo-of-a-sheet-of-pape</a>
- <a href="https://answers.opencv.org/question/75510/how-to-make-auto-adjustmentsbrightness-and-contrast-for-image-android-opencv-image-correction/">https://answers.opencv.org/question/75510/how-to-make-auto-adjustmentsbrightness-and-contrast-for-image-android-opencv-image-correction/</a>
- <a href="https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#convertscaleabs">https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#convertscaleabs</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def automatic_brightness_and_contrast(image, clip_hist_percent=1):
    &#34;&#34;&#34;Adjust automatically brightness and contrast of the image.

    Brightness and contrast is linear operator with parameter alpha and beta:
        g(x,y)= α * f(x,y)+ β

    It is recommended to visit the first link in the notes.

    Parameters
    ----------
    img: ndarray
        the input image

    Returns
    -------
    tuple
        new_img = adjusted image,
        alpha = alpha value calculated,
        beta = beta value calculated

    Notes
    -----
    For details visit.
    - https://stackoverflow.com/questions/56905592/automatic-contrast-and-brightness-adjustment-of-a-color-photo-of-a-sheet-of-pape
    - https://answers.opencv.org/question/75510/how-to-make-auto-adjustmentsbrightness-and-contrast-for-image-android-opencv-image-correction/
    - https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#convertscaleabs
    &#34;&#34;&#34;
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Calculate grayscale histogram
    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
    hist_size = len(hist)

    # Calculate cumulative distribution from the histogram
    accumulator = []
    accumulator.append(float(hist[0]))
    for index in range(1, hist_size):
        accumulator.append(accumulator[index - 1] + float(hist[index]))

    # Locate points to clip
    maximum = accumulator[-1]
    clip_hist_percent *= (maximum / 100.0)
    clip_hist_percent /= 2.0

    # Locate left cut
    minimum_gray = 0
    while accumulator[minimum_gray] &lt; clip_hist_percent:
        minimum_gray += 1

    # Locate right cut
    maximum_gray = hist_size - 1
    while accumulator[maximum_gray] &gt;= (maximum - clip_hist_percent):
        maximum_gray -= 1

    # Calculate alpha and beta values
    alpha = 255 / (maximum_gray - minimum_gray)
    beta = -minimum_gray * alpha

    &#39;&#39;&#39;
    # Calculate new histogram with desired range and show histogram 
    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])
    plt.plot(hist)
    plt.plot(new_hist)
    plt.xlim([0,256])
    plt.show()
    &#39;&#39;&#39;

    new_img = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)
    return new_img, alpha, beta</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.canny_edge_detection"><code class="name flex">
<span>def <span class="ident">canny_edge_detection</span></span>(<span>img, threshold1, threshold2)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds edges in an image using the Canny algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the input image</dd>
<dt><strong><code>threshold1</code></strong> :&ensp;<code>int</code></dt>
<dd>first threshold for the hysteresis procedure.</dd>
<dt><strong><code>threshold2</code></strong> :&ensp;<code>int</code></dt>
<dd>second threshold for the hysteresis procedure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>returns an edge map that has the same size and type as <code>img</code></dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit:
- <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny#canny">https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny#canny</a>
- <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def canny_edge_detection(img, threshold1, threshold2):
    &#34;&#34;&#34;Finds edges in an image using the Canny algorithm.

    Parameters
    ----------
    img: ndarray
        the input image
    threshold1: int
        first threshold for the hysteresis procedure.
    threshold2: int
        second threshold for the hysteresis procedure.

    Returns
    -------
    ndarray
        returns an edge map that has the same size and type as `img`

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny#canny
    - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html
    &#34;&#34;&#34;
    return cv2.Canny(img, threshold1, threshold2)</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.create_segmented_image"><code class="name flex">
<span>def <span class="ident">create_segmented_image</span></span>(<span>img, contours)</span>
</code></dt>
<dd>
<div class="desc"><p>Create an image the the contours are white filled and the rest is black.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>input image necessary to the shape</dd>
<dt><strong><code>contours</code></strong> :&ensp;<code>list</code></dt>
<dd>list of contours. Each individual contour is a Numpy array
of (x,y) coordinates of boundary points of the object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>the segmented image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_segmented_image(img, contours):
    &#34;&#34;&#34;
    Create an image the the contours are white filled and the rest is black.

    Parameters
    ----------
    img: ndarray
        input image necessary to the shape
    contours: list
        list of contours. Each individual contour is a Numpy array
        of (x,y) coordinates of boundary points of the object.

    Returns
    -------
    ndarray
        the segmented image
    &#34;&#34;&#34;

    h = img.shape[0]
    w = img.shape[1]

    segmented = np.zeros((h, w), dtype=np.uint8)

    cv2.drawContours(segmented, contours, -1, 255, cv2.FILLED)

    return segmented</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.extend_image_lines"><code class="name flex">
<span>def <span class="ident">extend_image_lines</span></span>(<span>img, lines, probabilistic_mode, color_value=255)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a mask by extending the lines received.</p>
<p>Create a mask of the same size of the image <code>img</code>, where the <code>lines</code> received
have been drawn in order to cross the whole image. The color used to draw
the lines is specified by <code>color_value</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the input image</dd>
<dt><strong><code>lines</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>a NumPy.array of lines</dd>
<dt><strong><code>probabilistic_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether to use the Standard (False) or the Probabilistic
(True) Hough Transform</dd>
<dt><strong><code>color_value</code></strong> :&ensp;<code>tuple</code></dt>
<dd>tuple (B,G,R) that specifies the color of the lines</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Returns the mask with the lines drawn.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit:
- <a href="https://answers.opencv.org/question/2966/how-do-the-rho-and-theta-values-work-in-houghlines/#:~:text=rho%20is%20the%20distance%20from,are%20called%20rho%20and%20theta.">https://answers.opencv.org/question/2966/how-do-the-rho-and-theta-values-work-in-houghlines/#:~:text=rho%20is%20the%20distance%20from,are%20called%20rho%20and%20theta.</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend_image_lines(img, lines, probabilistic_mode, color_value=255):
    &#34;&#34;&#34;Create a mask by extending the lines received.

    Create a mask of the same size of the image `img`, where the `lines` received
    have been drawn in order to cross the whole image. The color used to draw
    the lines is specified by `color_value`.

    Parameters
    ----------
    img: ndarray
        the input image
    lines: ndarray
        a NumPy.array of lines
    probabilistic_mode: bool
        determines whether to use the Standard (False) or the Probabilistic
        (True) Hough Transform
    color_value: tuple
        tuple (B,G,R) that specifies the color of the lines

    Returns
    -------
    ndarray
        Returns the mask with the lines drawn.

    Notes
    -----
    For details visit:
    - https://answers.opencv.org/question/2966/how-do-the-rho-and-theta-values-work-in-houghlines/#:~:text=rho%20is%20the%20distance%20from,are%20called%20rho%20and%20theta.
    &#34;&#34;&#34;

    h = img.shape[0]
    w = img.shape[1]

    mask = np.zeros((h, w), dtype=np.uint8)

    length = np.max((h, w))

    for line in lines:
        line = line[0]
        if probabilistic_mode:
            theta = np.arctan2(line[1] - line[3], line[0] - line[2])

            x0 = line[0]
            y0 = line[1]

            a = np.cos(theta)
            b = np.sin(theta)

            pt1 = (int(x0 - length * a), int(y0 - length * b),)
            pt2 = (int(x0 + length * a), int(y0 + length * b),)
        else:
            rho = line[0]
            theta = line[1]

            a = np.cos(theta)
            b = np.sin(theta)

            # Read: https://answers.opencv.org/question/2966/how-do-the-rho-and-theta-values-work-in-houghlines/#:~:text=rho%20is%20the%20distance%20from,are%20called%20rho%20and%20theta.
            x0 = a * rho
            y0 = b * rho

            length = 40000

            pt1 = (int(x0 + length * (-b)), int(y0 + length * (a)))
            pt2 = (int(x0 - length * (-b)), int(y0 - length * (a)))

        cv2.line(mask, pt1, pt2, color_value, 2, cv2.LINE_AA)  # cv2.LINE_AA

    return mask</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.find_corners"><code class="name flex">
<span>def <span class="ident">find_corners</span></span>(<span>img, max_number_corners=4, corner_quality=0.001, min_distance=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform Shi-Tomasi Corner detection.</p>
<p>Perform Shi-Tomasi Corner detection to return the corners found in the image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the input image</dd>
<dt><strong><code>max_number_corners</code></strong> :&ensp;<code>int</code></dt>
<dd>maximum number of corners to return</dd>
<dt><strong><code>corner_quality</code></strong> :&ensp;<code>float</code></dt>
<dd>minimal accepted quality of image corners. The corners with the quality
measure less than the product are rejected.</dd>
<dt><strong><code>min_distance</code></strong> :&ensp;<code>int</code></dt>
<dd>minimum Euclidean distance between the returned corners</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Returns a NumPy array of the most prominent corners in the image, in the
form (x,y).</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit:
- <a href="https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541">https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541</a>
- <a href="https://docs.opencv.org/master/d4/d8c/tutorial_py_shi_tomasi.html">https://docs.opencv.org/master/d4/d8c/tutorial_py_shi_tomasi.html</a>
- <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_corners(img, max_number_corners=4, corner_quality=0.001, min_distance=20):
    &#34;&#34;&#34;Perform Shi-Tomasi Corner detection.

    Perform Shi-Tomasi Corner detection to return the corners found in the image.

    Parameters
    ----------
    img: ndarray
        the input image
    max_number_corners: int
        maximum number of corners to return
    corner_quality: float
        minimal accepted quality of image corners. The corners with the quality
        measure less than the product are rejected.
    min_distance: int
        minimum Euclidean distance between the returned corners

    Returns
    -------
    ndarray
        Returns a NumPy array of the most prominent corners in the image, in the
        form (x,y).

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541
    - https://docs.opencv.org/master/d4/d8c/tutorial_py_shi_tomasi.html
    - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html
    &#34;&#34;&#34;

    corners = cv2.goodFeaturesToTrack(
        img,
        max_number_corners,
        corner_quality,
        min_distance
    )

    return corners</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.find_hough_lines"><code class="name flex">
<span>def <span class="ident">find_hough_lines</span></span>(<span>img, probabilistic_mode=False, rho=1, theta=0.017453292519943295, threshold=0, ratio_percentage=0.15)</span>
</code></dt>
<dd>
<div class="desc"><p>Detect straight lines.</p>
<p>Detect straight lines using the Standard or Probabilistic Hough
Line Transform.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>input image</dd>
<dt><strong><code>probabilistic_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>determines whether to use the Standard (False) or the Probabilistic
(True) Hough Transform</dd>
<dt><strong><code>rho</code></strong> :&ensp;<code>int</code></dt>
<dd>distance resolution of the accumulator in pixels.</dd>
<dt><strong><code>theta</code></strong> :&ensp;<code>float</code></dt>
<dd>angle resolution of the accumulator in radians.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>int</code></dt>
<dd>accumulator threshold parameter. Only those rows that get enough
votes ( &gt;<code>threshold</code> ) are returned.</dd>
<dt><strong><code>ratio_percentage</code></strong> :&ensp;<code>float</code></dt>
<dd>percentage of the image's larger side. The image is searched for
lines who's length is at least a certain percentage of the image's
larger side (default 15%).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Returns a NumPy.array of lines</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit:
- <a href="https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html">https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html</a>
- <a href="https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a">https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_hough_lines(img, probabilistic_mode=False, rho=1, theta=np.pi / 180, threshold=0, ratio_percentage=0.15):
    &#34;&#34;&#34;Detect straight lines.

    Detect straight lines using the Standard or Probabilistic Hough
    Line Transform.

    Parameters
    ----------
    img: ndarray
        input image
    probabilistic_mode: bool
        determines whether to use the Standard (False) or the Probabilistic
        (True) Hough Transform
    rho: int
        distance resolution of the accumulator in pixels.
    theta: float
        angle resolution of the accumulator in radians.
    threshold: int
        accumulator threshold parameter. Only those rows that get enough
        votes ( &gt;`threshold` ) are returned.
    ratio_percentage: float
        percentage of the image&#39;s larger side. The image is searched for
        lines who&#39;s length is at least a certain percentage of the image&#39;s
        larger side (default 15%).

    Returns
    -------
    ndarray
        Returns a NumPy.array of lines

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html
    - https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a
    &#34;&#34;&#34;

    h, w = img.shape
    if probabilistic_mode:
        img_ratio = np.max([h, w]) * ratio_percentage
        lines = cv2.HoughLinesP(img, rho, theta, threshold, img_ratio, img_ratio / 3.5)
    else:
        lines = cv2.HoughLines(img, rho, theta, threshold, None, 0, 0)

    return lines</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.find_image_contours"><code class="name flex">
<span>def <span class="ident">find_image_contours</span></span>(<span>img, mode, method)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds contours in a binary image.</p>
<p>The function retrieves contours from a binary image (i.e. the wall mask we
find before)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>binary image in which to find the contours (i.e. the wall mask)</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>int</code></dt>
<dd>Contour retrieval mode</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>int</code></dt>
<dd>Contour approximation method</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>contours</code></strong> :&ensp;<code>list</code></dt>
<dd>Detected contours. Each contour is stored as a list of all the
contours in the image. Each individual contour is a Numpy array
of (x,y) coordinates of boundary points of the object.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Fot details visit:
- <a href="https://docs.opencv.org/trunk/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0">https://docs.opencv.org/trunk/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0</a>
- <a href="https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html">https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_image_contours(img, mode, method):
    &#34;&#34;&#34;Finds contours in a binary image.

    The function retrieves contours from a binary image (i.e. the wall mask we
    find before)

    Parameters
    ----------
    img: ndarray
        binary image in which to find the contours (i.e. the wall mask)
    mode: int
        Contour retrieval mode
    method: int
        Contour approximation method

    Returns
    -------
    contours: list
        Detected contours. Each contour is stored as a list of all the
        contours in the image. Each individual contour is a Numpy array
        of (x,y) coordinates of boundary points of the object.

    Notes
    -----
    Fot details visit:
    - https://docs.opencv.org/trunk/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0
    - https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html
    &#34;&#34;&#34;

    contours, hierarchy = cv2.findContours(img, mode, method)

    return contours, hierarchy</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.find_largest_segment"><code class="name flex">
<span>def <span class="ident">find_largest_segment</span></span>(<span>img, color_difference=1, x_samples=8)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a mask using the largest segment (this segment will be white).</p>
<p>This is done by setting every pixel that is not the same color of the wall
to have a value of 0 and every pixel has a value within a euclidean distance
of <code>color_difference</code> to the wall's pixel value to have a value of 255.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>image to apply masking</dd>
<dt><strong><code>color_difference</code></strong> :&ensp;<code>int</code></dt>
<dd>euclidean distance between wall's pixel and the rest of the image</dd>
<dt><strong><code>x_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>numer of samples that will be tested orizontally in the image</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Returns a version of the image where the wall is white and the rest of
the image is black.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_largest_segment(img, color_difference=1, x_samples=8):
    &#34;&#34;&#34;Create a mask using the largest segment (this segment will be white).

    This is done by setting every pixel that is not the same color of the wall
    to have a value of 0 and every pixel has a value within a euclidean distance
    of `color_difference` to the wall&#39;s pixel value to have a value of 255.

    Parameters
    ----------
    img: ndarray
        image to apply masking
    color_difference: int
        euclidean distance between wall&#39;s pixel and the rest of the image
    x_samples: int
        numer of samples that will be tested orizontally in the image

    Returns
    -------
    ndarray
        Returns a version of the image where the wall is white and the rest of
        the image is black.
    &#34;&#34;&#34;

    h, w, chn = img.shape
    color_difference = (color_difference,) * 3

    # in that way for smaller images the stride will be lower
    stride = int(w / x_samples)

    mask = np.zeros((h + 2, w + 2), dtype=np.uint8)
    wall_mask = mask
    largest_segment = 0
    for y in range(0, h, stride):
        for x in range(0, w, stride):
            if mask[y + 1, x + 1] == 0:
                mask[:] = 0
                # Fills a connected component with the given color.
                # For details visit:
                # https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html?highlight=floodfill#floodfill
                rect = cv2.floodFill(
                    image=img.copy(),
                    mask=mask,
                    seedPoint=(x, y),
                    newVal=0,
                    loDiff=color_difference,
                    upDiff=color_difference,
                    flags=4 | (255 &lt;&lt; 8),
                )

                # For details visit:
                # https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57

                # Next operation is not necessary if flag is equal to `4 | ( 255 &lt;&lt; 8 )`
                # _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
                segment_size = mask.sum()
                if segment_size &gt; largest_segment:
                    largest_segment = segment_size
                    wall_mask = mask[1:-1, 1:-1].copy()
    #                     show_image(&#39;rect[2]&#39;, mask, height=405, width=720)
    # cv2.waitKey(0)
    return wall_mask</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.image_blurring"><code class="name flex">
<span>def <span class="ident">image_blurring</span></span>(<span>img, ksize)</span>
</code></dt>
<dd>
<div class="desc"><p>Blurs an image using the median filter.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the input image</dd>
<dt><strong><code>ksize</code></strong> :&ensp;<code>int</code></dt>
<dd>aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 &hellip;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>the image blurred</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit:
- <a href="https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9">https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9</a>
- <a href="https://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html">https://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def image_blurring(img, ksize):
    &#34;&#34;&#34;Blurs an image using the median filter.

    Parameters
    ----------
    img: ndarray
        the input image
    ksize: int
        aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...

    Returns
    -------
    ndarray
        the image blurred

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9
    - https://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html
    &#34;&#34;&#34;

    assert ksize &gt; 1 and ksize % 2 != 0, &#34;`ksize` should be odd and grater than 1&#34;
    return cv2.medianBlur(img, ksize)</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.image_dilation"><code class="name flex">
<span>def <span class="ident">image_dilation</span></span>(<span>img, kernel_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Dilate the image.</p>
<p>Dilation involves moving a kernel over the pixels of a binary image. When
the kernel is centered on a pixel with a value of 0 and some of its pixels
are on pixels with a value of 1, the centre pixel is given a value of 1.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the img to be dilated</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>the kernel size</dd>
<dt><strong><code>kernel_value</code></strong> :&ensp;<code>int</code></dt>
<dd>the kernel value (the value of each kernel pixel)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Returns the dilated img</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def image_dilation(img, kernel_size):
    &#34;&#34;&#34;Dilate the image.

    Dilation involves moving a kernel over the pixels of a binary image. When
    the kernel is centered on a pixel with a value of 0 and some of its pixels
    are on pixels with a value of 1, the centre pixel is given a value of 1.

    Parameters
    ----------
    img: ndarray
        the img to be dilated
    kernel_size: int
        the kernel size
    kernel_value: int
        the kernel value (the value of each kernel pixel)

    Returns
    -------
    ndarray
        Returns the dilated img
    &#34;&#34;&#34;

    kernel = np.ones((kernel_size, kernel_size))
    dilated_img = cv2.dilate(img, kernel)
    return dilated_img</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.image_erosion"><code class="name flex">
<span>def <span class="ident">image_erosion</span></span>(<span>img, kernel_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Erode the image.</p>
<p>It's the opposite of dilation. Erosion involves moving a kernel over the
pixels of a binary image. A pixel in the original image (either 1 or 0)
will be considered 1 only if all the pixels under the kernel is 1,
otherwise it is eroded (made to zero).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the img to be eroded</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>the kernel size</dd>
<dt><strong><code>kernel_value</code></strong> :&ensp;<code>int</code></dt>
<dd>the kernel value (the value of each kernel pixel)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Returns the eroded image</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit:
- <a href="https://docs.opencv.org/trunk/d9/d61/tutorial_py_morphological_ops.html">https://docs.opencv.org/trunk/d9/d61/tutorial_py_morphological_ops.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def image_erosion(img, kernel_size):
    &#34;&#34;&#34;Erode the image.

    It&#39;s the opposite of dilation. Erosion involves moving a kernel over the
    pixels of a binary image. A pixel in the original image (either 1 or 0)
    will be considered 1 only if all the pixels under the kernel is 1,
    otherwise it is eroded (made to zero).

    Parameters
    ----------
    img: ndarray
        the img to be eroded
    kernel_size: int
        the kernel size
    kernel_value: int
        the kernel value (the value of each kernel pixel)

    Returns
    -------
    ndarray
        Returns the eroded image

    Notes
    -----
    For details visit:
    - https://docs.opencv.org/trunk/d9/d61/tutorial_py_morphological_ops.html
    &#34;&#34;&#34;

    kernel = np.ones((kernel_size, kernel_size))
    eroded_img = cv2.erode(img, kernel)
    return eroded_img</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.image_morphology_tranformation"><code class="name flex">
<span>def <span class="ident">image_morphology_tranformation</span></span>(<span>img, operation, kernel_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs morphological transformations</p>
<p>Performs morphological transformations of the image using an erosion
and dilation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the input image</dd>
<dt><strong><code>operation</code></strong> :&ensp;<code>int</code></dt>
<dd>type of operation</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>the kernel size</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>the transformed image of the same size and type as source image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def image_morphology_tranformation(img, operation, kernel_size):
    &#34;&#34;&#34;Performs morphological transformations

    Performs morphological transformations of the image using an erosion
    and dilation.

    Parameters
    ----------
    img: ndarray
        the input image
    operation: int
        type of operation
    kernel_size: int
        the kernel size

    Returns
    -------
    ndarray
        the transformed image of the same size and type as source image

    &#34;&#34;&#34;
    kernel = np.ones((kernel_size, kernel_size))
    transformed_img = cv2.morphologyEx(img, operation, kernel)
    return transformed_img</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.image_resize"><code class="name flex">
<span>def <span class="ident">image_resize</span></span>(<span>img, width, height, interpolation=2)</span>
</code></dt>
<dd>
<div class="desc"><p>Resize the input image to the given size.</p>
<p>Resize the input image to the given size using the given interpolation
method.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the input image</dd>
<dt><strong><code>width</code></strong> :&ensp;<code>int</code></dt>
<dd>width of the target resized image</dd>
<dt><strong><code>height</code></strong> :&ensp;<code>int</code></dt>
<dd>height of the target resized image</dd>
<dt><strong><code>interpolation</code></strong> :&ensp;<code>int</code></dt>
<dd>interpolation method</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>(ndarray, float) = (the resized image, the scale factor)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def image_resize(img, width, height, interpolation=cv2.INTER_CUBIC):
    &#34;&#34;&#34;Resize the input image to the given size.

    Resize the input image to the given size using the given interpolation
    method.

    Parameters
    ----------
    img: ndarray
        the input image
    width: int
        width of the target resized image
    height: int
        height of the target resized image
    interpolation: int
        interpolation method

    Returns
    -------
    tuple
        (ndarray, float) = (the resized image, the scale factor)
    &#34;&#34;&#34;

    scale_factor = 1.
    resized_img = img
    h_img, w_img, c_img = img.shape

    if h_img &gt; height and w_img &gt; width:
        scale_factor = h_img / height
        height_scaled = height
        width_scaled = width
        resized_img = cv2.resize(img, (width_scaled, height_scaled), interpolation)

    return resized_img, scale_factor</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.invert_image"><code class="name flex">
<span>def <span class="ident">invert_image</span></span>(<span>img)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns an inverted version of the image.</p>
<p>The function calculates per-element bit-wise inversion of the input image.
This means that black (=0) pixels become white (=255), and vice versa.</p>
<p>In our case, we need to invert the wall mask for finding possible painting components.</p>
<p>From OpenCV documentation (<a href="https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html">https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html</a>):
"
In OpenCV, finding contours is like finding white object from black
background. So remember, object to be found should be white and background
should be black.
"</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the image to invert</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>the inverted image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def invert_image(img):
    &#34;&#34;&#34;Returns an inverted version of the image.

    The function calculates per-element bit-wise inversion of the input image.
    This means that black (=0) pixels become white (=255), and vice versa.

    In our case, we need to invert the wall mask for finding possible painting components.

    From OpenCV documentation (https://docs.opencv.org/trunk/d4/d73/tutorial_py_contours_begin.html):
    &#34;
        In OpenCV, finding contours is like finding white object from black
        background. So remember, object to be found should be white and background
        should be black.
    &#34;

    Parameters
    ----------
    img: ndarray
        the image to invert

    Returns
    -------
    ndarray
        the inverted image
    &#34;&#34;&#34;

    return cv2.bitwise_not(img)</code></pre>
</details>
</dd>
<dt id="code.tasks.image_processing.mean_shift_segmentation"><code class="name flex">
<span>def <span class="ident">mean_shift_segmentation</span></span>(<span>img, spatial_radius, color_radius, maximum_pyramid_level)</span>
</code></dt>
<dd>
<div class="desc"><p>Groups pixels together by colour and location.</p>
<p>This function takes an image and mean-shift parameters and returns a version
of the image that has had mean shift segmentation performed on it.</p>
<p>Mean shift segmentation clusters nearby pixels with similar pixel values and
sets them all to have the value of the local maxima of pixel value.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>image to apply the Mean Shift Segmentation</dd>
<dt><strong><code>spatial_radius</code></strong> :&ensp;<code>int</code></dt>
<dd>The spatial window radius</dd>
<dt><strong><code>color_radius</code></strong> :&ensp;<code>int</code></dt>
<dd>The color window radius</dd>
<dt><strong><code>maximum_pyramid_level</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum level of the pyramid for the segmentation</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>filtered “posterized” image with color gradients and fine-grain
texture flattened</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For details visit:
- <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering">https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering</a>
- <a href="https://docs.opencv.org/master/d7/d00/tutorial_meanshift.html">https://docs.opencv.org/master/d7/d00/tutorial_meanshift.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_shift_segmentation(img, spatial_radius, color_radius, maximum_pyramid_level):
    &#34;&#34;&#34;Groups pixels together by colour and location.

    This function takes an image and mean-shift parameters and returns a version
    of the image that has had mean shift segmentation performed on it.

    Mean shift segmentation clusters nearby pixels with similar pixel values and
    sets them all to have the value of the local maxima of pixel value.

    Parameters
    ----------
    img: ndarray
        image to apply the Mean Shift Segmentation
    spatial_radius: int
        The spatial window radius
    color_radius: int
        The color window radius
    maximum_pyramid_level: int
        Maximum level of the pyramid for the segmentation

    Returns
    -------
    ndarray
        filtered “posterized” image with color gradients and fine-grain
        texture flattened

    Notes
    -------
    For details visit:
    - https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#pyrmeanshiftfiltering
    - https://docs.opencv.org/master/d7/d00/tutorial_meanshift.html

    &#34;&#34;&#34;

    dst_img = cv2.pyrMeanShiftFiltering(img, spatial_radius, color_radius, maximum_pyramid_level)
    return dst_img</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="code.tasks" href="index.html">code.tasks</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="code.tasks.image_processing.automatic_brightness_and_contrast" href="#code.tasks.image_processing.automatic_brightness_and_contrast">automatic_brightness_and_contrast</a></code></li>
<li><code><a title="code.tasks.image_processing.canny_edge_detection" href="#code.tasks.image_processing.canny_edge_detection">canny_edge_detection</a></code></li>
<li><code><a title="code.tasks.image_processing.create_segmented_image" href="#code.tasks.image_processing.create_segmented_image">create_segmented_image</a></code></li>
<li><code><a title="code.tasks.image_processing.extend_image_lines" href="#code.tasks.image_processing.extend_image_lines">extend_image_lines</a></code></li>
<li><code><a title="code.tasks.image_processing.find_corners" href="#code.tasks.image_processing.find_corners">find_corners</a></code></li>
<li><code><a title="code.tasks.image_processing.find_hough_lines" href="#code.tasks.image_processing.find_hough_lines">find_hough_lines</a></code></li>
<li><code><a title="code.tasks.image_processing.find_image_contours" href="#code.tasks.image_processing.find_image_contours">find_image_contours</a></code></li>
<li><code><a title="code.tasks.image_processing.find_largest_segment" href="#code.tasks.image_processing.find_largest_segment">find_largest_segment</a></code></li>
<li><code><a title="code.tasks.image_processing.image_blurring" href="#code.tasks.image_processing.image_blurring">image_blurring</a></code></li>
<li><code><a title="code.tasks.image_processing.image_dilation" href="#code.tasks.image_processing.image_dilation">image_dilation</a></code></li>
<li><code><a title="code.tasks.image_processing.image_erosion" href="#code.tasks.image_processing.image_erosion">image_erosion</a></code></li>
<li><code><a title="code.tasks.image_processing.image_morphology_tranformation" href="#code.tasks.image_processing.image_morphology_tranformation">image_morphology_tranformation</a></code></li>
<li><code><a title="code.tasks.image_processing.image_resize" href="#code.tasks.image_processing.image_resize">image_resize</a></code></li>
<li><code><a title="code.tasks.image_processing.invert_image" href="#code.tasks.image_processing.invert_image">invert_image</a></code></li>
<li><code><a title="code.tasks.image_processing.mean_shift_segmentation" href="#code.tasks.image_processing.mean_shift_segmentation">mean_shift_segmentation</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.3</a>.</p>
</footer>
</body>
</html>